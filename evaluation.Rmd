---
title: "Evaluation"
---
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = FALSE,
	message = FALSE,
	warning = FALSE
)
  species <- rodent_spp(set = "evalplot")
  fcasts <- read_casts() %>%
            select_casts(species = species, level = "Controls") %>%
            append_observed_to_cast()
  fcast_errors <- measure_cast_error(fcasts)
  hcasts <- read_casts(cast_type = "hindcasts") %>%
            select_casts(species = species, level = "Controls") %>%
            append_observed_to_cast()
  hcast_errors <- measure_cast_error(hcasts)
```

## Metrics
**Error**: Error is the raw difference between a predicted point value ($\hat{y}_{i}$) and observed value ($y_{i}$): $e_{i} = \hat{y}_{i} - y_{i}$. Although we generally summarize errors in a fashion that results in non-negative values only (*e.g.*, via squaring), the raw error provides information about prediction bias (if the models tend to over- or under-predict in a systematic fashion).

**RMSE**: Root Mean Square Error (RMSE) is used to evaluate the absolute accuracy of the point estimates of a forecast. Across $N$ predicted values in a time series, $RMSE = \sqrt{\frac{\sum_{i=1}^{N}{e_{i}^2}}{N}}$.  

**Coverage**: Coverage is used to evaluate the uncertainty in a forecast and is the percentage of observations which fell within the confidence interval of the prediction. Because we use a 90% CI, coverage would ideally be equal to 0.90. If coverage is higher than 0.90, the forecast's intervals are too wide; if it's lower than 0.90, the forecast intervals are too narrow.

## How have the models done recently?


## How have the models done historically?

These graphs show errors as a function of lead time. The lead time is the number of months into the future that forecast is made. The error values are an average of all forecast errors using observations since 2010. Note that this currently uses hindcasts of the prior observations, and is also only for the Control plots.



