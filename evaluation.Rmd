---
title: "Evaluation"
---
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = FALSE,
	message = FALSE,
	warning = FALSE
)
  species <- rodent_spp(set = "evalplot")
  fcasts <- read_casts() %>%
            select_casts(species = species, level = "Controls") %>%
            append_observed_to_cast()
  fcast_errors <- measure_cast_error(fcasts)
  hcasts <- read_casts(cast_type = "hindcasts") %>%
            select_casts(species = species, level = "Controls") %>%
            append_observed_to_cast()
  hcast_errors <- measure_cast_error(hcasts)
```

## Metrics
**Error**: Error is the raw difference between a predicted value ($\hat{y}_{i}$) and observed value ($y_{i}$): 
**RMSE**: Root Mean Square Error (RMSE) is a metric used to evaluate the point estimate of a forecast.  
**Coverage**: Coverage is used to evaluate the uncertainty in a forecast and is the percentage of observations which fell within the confidence interval of the prediction. Because we use a 90% CI, coverage would ideally be equal to 0.90. If coverage is higher than 0.90, the forecast's intervals are too wide; if it's lower than 0.90, the forecast intervals are too narrow.

## How have the models done recently?


## How have the models done historically?

These graphs show errors as a function of lead time. The lead time is the number of months into the future that forecast is made. The error values are an average of all forecast errors using observations since 2010. Note that this currently uses hindcasts of the prior observations, and is also only for the Control plots.



